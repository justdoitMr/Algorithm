## 模型评估

有三种不同的方法来评估一个模型的预测质量：

1. estimator的score方法：sklearn中的estimator都具有一个score方法，它提供了一个缺省的评估法则来解决问题。
2. Scoring参数：使用cross-validation的模型评估工具，依赖于内部的scoring策略。见下。
3. 通过测试集上评估预测误差：sklearn Metric函数用来评估预测误差。

## 评价指标

评价指标针对不同的机器学习任务有不同的指标，同一任务也有不同侧重点的评价指标。

1. 主要有分类（classification）

2. 回归（regression）

3. 排序（ranking）

4. 聚类（clustering）

5. 热门主题模型（topic modeling）

6. 推荐（recommendation）

## 分类评价指标

### accuracy_score(y_true,y_pre) : 

是模型分类正确的数据除以样本总数 **【模型的score方法算的也是准确率】**

 ~~~ python
accuracy_score(y_test,y_pre)
# 或者 model.score(x_test,y_test)，大多模型都是有score方法的
 ~~~

### precision_score(精确度)、recall_score(召回率)、**f1_score**（后者由前两个推导出的）

#### 精确度

> average_precision_score(*y_true*, *y_score*, *average='macro'*, *sample_weight=None*)
>
>  precision_score(*y_true*, *y_pred*, *labels=None*, *pos_label=1*, *average='binary'*,) ：查准率或者精度； precision(查准率)=TP/(TP+FP)

#### f1_score

> f1_score(*y_true*, *y_pred*, *labels=None*, *pos_label=1*, *average='binary'*, *sample_weight=None*)
>
> F1值:　F1 = 2 * (precision * recall) / (precision + recall) precision(查准率)=TP/(TP+FP) recall(查全率)=TP/(TP+FN)

#### 召回率(查全率)

> recall_score(*y_true*, *y_pred*, *labels=None*, *pos_label=1*, *average='binary'*, *sample_weight=None*)
>
> recall(查全率)=TP/(TP+FN)

　这三个不仅适合二分类，**也适合多分类。只需要指出参数average=‘micro’/‘macro’/'weighted’**

- macro：计算二分类metrics的均值，为每个类给出相同权重的分值。当小类很重要时会出问题，因为该macro-averging方法是对性能的平均。另一方面，该方法假设所有分类都是一样重要的，因此macro-averaging方法会对小类的性能影响很大
- micro： 给出了每个样本类以及它对整个metrics的贡献的pair（sample-weight），而非对整个类的metrics求和，它会每个类的metrics上的权重及因子进行求和，来计算整个份额。Micro-averaging方法在多标签（multilabel）问题中设置，包含多分类，此时，大类将被忽略
- weighted: 对于不均衡数量的类来说，计算二分类metrics的平均，通过在每个类的score上进行加权实现

### brier_score_loss(*y_true*, *y_prob*, *sample_weight=None*, *pos_label=None*)

The smaller the Brier score, the better.

### confusion_matrix(*y_true*, *y_pred*, *labels=None*, *sample_weight=None*)

通过计算混淆矩阵来评估分类的准确性 返回混淆矩阵

### log_loss(*y_true*, *y_pred*, *eps=1e-15*, *normalize=True*, *sample_weight=None*, *labels=None*)

对数损耗，又称逻辑损耗或交叉熵损耗

### roc_auc_score(*y_true*, *y_score*, *average='macro'*, *sample_weight=None*)：

计算ROC曲线下的面积就是AUC的值，the larger the better

### auc(x, *y*, *reorder=False*) 

ROC曲线下的面积;较大的AUC代表了较好的performance

### roc_curve(*y_true*, *y_score*, *pos_label=None*, *sample_weight=None*, *drop_intermediate=True*)；

计算ROC曲线的横纵坐标值，TPR，FPR

TPR = TP/(TP+FN) = recall(真正例率，敏感度)       FPR = FP/(FP+TN)(假正例率，1-特异性)

### 回归指标

常见的损失MAE、MSE、R^2

### explained_variance_score(*y_true*, *y_pred*, *sample_weight=None*, *multioutput='uniform_average'*)

回归方差(反应自变量与因变量之间的相关程度)

 

### mean_absolute_error(*y_true*, *y_pred*, *sample_weight=None*, *multioutput='uniform_average'*)

mean_absolute_error（MAE、平均绝对误差）



 

### mean_squared_error(*y_true*, *y_pred*, *sample_weight=None*, *multioutput='uniform_average'*)

**mean_squared_error**（MSE、均方误差）

 

### median_absolute_error(*y_true*, *y_pred*)   

中值绝对误差

 

### r2_score(*y_true*, *y_pred*, *sample_weight=None*, *multioutput='uniform_average'*)  

**r2_score**（R^2、可决系数）

## 交叉验证中指定scoring参数

**交叉验证cross_val_score的scoring参数**

- 分类：accuracy(准确率)、f1、f1_micro、f1_macro（这两个用于多分类的f1_score）、precision(精确度)、recall(召回率)、roc_auc
- 回归：neg_mean_squared_error（MSE、均方误差）、r2
- 聚类：adjusted_rand_score、completeness_score等 【这一块我没怎么用过】